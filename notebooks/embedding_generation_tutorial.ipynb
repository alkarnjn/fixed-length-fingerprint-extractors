{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3abf8055",
   "metadata": {},
   "source": [
    "To execute this notebook, you need to either\n",
    " - Download a pre-trained model\n",
    " - Train an example model by excecuting the model_training_tutorial.ipynb notebook\n",
    "\n",
    "In this tutorial we will learn to:\n",
    "- Load a previously trained model\n",
    "- Extract DeepPrint features from fingerprint images\n",
    "- Evaluate the performance of the extracted fixed-length representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c282c13c",
   "metadata": {},
   "source": [
    "## Embedding extraction\n",
    "\n",
    "After training the model, we can extract the DeepPrint features for the fingerprint images. This is done by calling the `extract` method of the `DeepPrintExtractor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f566250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from /home/mt0/22CS60R42/dataset/fvc2006db3_A_model/best_model.pyt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"filename 'storages' not found\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# To load the pre-trained model parameters use\u001b[39;00m\n\u001b[1;32m     11\u001b[0m MODEL_DIR: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mt0/22CS60R42/dataset/fvc2006db3_A_model\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Path to the directory containing the model parameters\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m extractor\u001b[38;5;241m.\u001b[39mload_best_model(MODEL_DIR)\n",
      "File \u001b[0;32m~/fixed-length-fingerprint-extractors/flx/extractor/fixed_length_extractor.py:68\u001b[0m, in \u001b[0;36mDeepPrintExtractor.load_best_model\u001b[0;34m(self, model_dir)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(model_path):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     load_model_parameters(model_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best model file found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/fixed-length-fingerprint-extractors/flx/models/torch_helpers.py:84\u001b[0m, in \u001b[0;36mload_model_parameters\u001b[0;34m(full_param_path, model, loss, optim)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(full_param_path):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_param_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m did not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(full_param_path, map_location\u001b[38;5;241m=\u001b[39mget_device())\n\u001b[1;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.anaconda3/lib/python3.11/site-packages/torch/serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.anaconda3/lib/python3.11/site-packages/torch/serialization.py:1231\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f_should_read_directly \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;66;03m# legacy_load requires that f has fileno()\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;66;03m# only if offset is zero we can attempt the legacy tar file loader\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m legacy_load(f)\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mTarError:\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(f):\n\u001b[1;32m   1234\u001b[0m             \u001b[38;5;66;03m# .zip is used for torch.jit.save and will throw an un-pickling error here\u001b[39;00m\n",
      "File \u001b[0;32m~/.anaconda3/lib/python3.11/site-packages/torch/serialization.py:1117\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.legacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m deserialized_objects[\u001b[38;5;28mint\u001b[39m(saved_id)]\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m closing(tarfile\u001b[38;5;241m.\u001b[39mopen(fileobj\u001b[38;5;241m=\u001b[39mf, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mtarfile\u001b[38;5;241m.\u001b[39mPAX_FORMAT)) \u001b[38;5;28;01mas\u001b[39;00m tar, \\\n\u001b[1;32m   1115\u001b[0m         mkdtemp() \u001b[38;5;28;01mas\u001b[39;00m tmpdir:\n\u001b[0;32m-> 1117\u001b[0m     tar\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorages\u001b[39m\u001b[38;5;124m'\u001b[39m, path\u001b[38;5;241m=\u001b[39mtmpdir)\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorages\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1119\u001b[0m         num_storages \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.anaconda3/lib/python3.11/tarfile.py:2091\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(member, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 2091\u001b[0m     tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetmember(member)\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2093\u001b[0m     tarinfo \u001b[38;5;241m=\u001b[39m member\n",
      "File \u001b[0;32m~/.anaconda3/lib/python3.11/tarfile.py:1813\u001b[0m, in \u001b[0;36mTarFile.getmember\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1811\u001b[0m tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getmember(name\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tarinfo\n",
      "\u001b[0;31mKeyError\u001b[0m: \"filename 'storages' not found\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from flx.extractor.fixed_length_extractor import get_DeepPrint_Tex, get_DeepPrint_TexMinu, DeepPrintExtractor\n",
    "\n",
    "# Dimension and number of training subjects must be known to load the pre-trained model\n",
    "\n",
    "# To load the pre-trained model parameters use num_training_subjects=8000\n",
    "extractor: DeepPrintExtractor = get_DeepPrint_TexMinu(num_training_subjects=140, num_dims=128)\n",
    "\n",
    "# To load the pre-trained model parameters use\n",
    "MODEL_DIR: str = os.path.abspath(\"/home/mt0/22CS60R42/dataset/fvc2006db3_A_model\") # Path to the directory containing the model parameters\n",
    "extractor.load_best_model(MODEL_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b33e2f8",
   "metadata": {},
   "source": [
    "Now we need to specify the dataset, for which we want to extract the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe661fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [12:02<00:00, 13.63s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from flx.data.dataset import *\n",
    "from flx.data.image_loader import SFingeLoader\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute path to the notebooks/example-dataset directory here! \n",
    "DATASET_PATH: str = os.path.abspath(\"/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/FVC2006DB2_A\")\n",
    "\n",
    "# We will use the SFingeLoader to load the images from the dataset\n",
    "image_loader = TransformedImageLoader(\n",
    "        images=SFingeLoader(DATASET_PATH),\n",
    "        poses=None,\n",
    "        transforms=[\n",
    "            LazilyAllocatedBinarizer(5.0),\n",
    "            pad_and_resize_to_deepprint_input_size,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "image_dataset: Dataset = Dataset(image_loader, image_loader.ids)\n",
    "\n",
    "# The second value is for the minutiae branch, which we do not have in this example\n",
    "texture_embeddings, minutia_embeddings = extractor.extract(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd4800fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04198745 -0.09042273  0.03778943 ...  0.05165511  0.05490642\n",
      "   0.03112722]\n",
      " [ 0.10568812 -0.0844481   0.07519364 ...  0.08187614  0.06651236\n",
      "  -0.00984057]\n",
      " [ 0.07251018 -0.07849421  0.07919195 ...  0.06697464  0.05448965\n",
      "   0.00292787]\n",
      " ...\n",
      " [ 0.04042731 -0.0387625   0.02640067 ... -0.03194454  0.06001967\n",
      "   0.00752258]\n",
      " [ 0.00854072 -0.02655505 -0.00838139 ... -0.0006599   0.06759997\n",
      "  -0.0119191 ]\n",
      " [ 0.04564777 -0.02835366  0.00805646 ... -0.02258757  0.06643154\n",
      "   0.01312804]]\n"
     ]
    }
   ],
   "source": [
    "texture_embeddings = extractor.get_DeepPrint_Tex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c15f7d98",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the embeddings, we want to run a benchmark on them. For this, we must first specify the type of benchmark, and which comparisons should be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4e4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:00<00:00, 7772.27it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7659.03it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7530.36it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7451.62it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 600.30it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7143.76it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7340.95it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7409.22it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7289.64it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7240.56it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7450.39it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 7307.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from flx.scripts.generate_benchmarks import create_verification_benchmark\n",
    "\n",
    "NUM_IMPRESSIONS_PER_SUBJECT = 12\n",
    "benchmark = create_verification_benchmark(\n",
    "    #subjects=list(range(100, image_dataset.num_subjects + 100)),\n",
    "    subjects=list(range(image_dataset.num_subjects)),\n",
    "    impressions_per_subject=list(range(NUM_IMPRESSIONS_PER_SUBJECT))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a37e9ef",
   "metadata": {},
   "source": [
    "Now we can run the benchmark. To do this, we must first specify the matcher (in our case cosine similarity of the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6bba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n",
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n",
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n",
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n",
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n",
      "Created IdentifierSet with 140 subjects and a total of 1680 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29400/29400 [00:00<00:00, 55984.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal-Error-Rate: 0.13839285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flx.benchmarks.matchers import CosineSimilarityMatcher\n",
    "from flx.data.embedding_loader import EmbeddingLoader\n",
    "\n",
    "# We concatenate texture and minutia embedding vectors\n",
    "embeddings = EmbeddingLoader.combine(texture_embeddings, minutia_embeddings)\n",
    "matcher = CosineSimilarityMatcher(EmbeddingLoader.combine(texture_embeddings, minutia_embeddings))\n",
    "\n",
    "results = benchmark.run(matcher)\n",
    "\n",
    "print(f\"Equal-Error-Rate: {results.get_equal_error_rate()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5ee4287",
   "metadata": {},
   "source": [
    "To visualize the results, we can plot a DET curve. (Do not wonder if it is empty, probably the model is not trained enough. Take a look at the EER instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.visualization.plot_DET_curve import plot_verification_results\n",
    "\n",
    "figure_path = \"DET_curve\"\n",
    "\n",
    "# Lists are used to allow for multiple models to be plotted in the same figure\n",
    "plot_verification_results(figure_path, results=[results], model_labels=[\"DeepPrint_TexMinu\"], plot_title=\"example-dataset - verification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
