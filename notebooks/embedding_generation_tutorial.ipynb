{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3abf8055",
   "metadata": {},
   "source": [
    "To execute this notebook, you need to either\n",
    " - Download a pre-trained model\n",
    " - Train an example model by excecuting the model_training_tutorial.ipynb notebook\n",
    "\n",
    "In this tutorial we will learn to:\n",
    "- Load a previously trained model\n",
    "- Extract DeepPrint features from fingerprint images\n",
    "- Evaluate the performance of the extracted fixed-length representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c282c13c",
   "metadata": {},
   "source": [
    "## Embedding extraction\n",
    "\n",
    "After training the model, we can extract the DeepPrint features for the fingerprint images. This is done by calling the `extract` method of the `DeepPrintExtractor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f566250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from flx.extractor.fixed_length_extractor import get_DeepPrint_Tex, get_DeepPrint_TexMinu, DeepPrintExtractor\n",
    "\n",
    "# Dimension and number of training subjects must be known to load the pre-trained model\n",
    "\n",
    "# To load the pre-trained model parameters use num_training_subjects=8000\n",
    "extractor: DeepPrintExtractor = get_DeepPrint_TexMinu(num_training_subjects=8000, num_dims=256)\n",
    "\n",
    "# To load the pre-trained model parameters use\n",
    "MODEL_DIR: str = os.path.abspath(\"/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/DeepPrint_TexMinu_512\") # Path to the directory containing the model parameters\n",
    "extractor.load_best_model(MODEL_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b33e2f8",
   "metadata": {},
   "source": [
    "Now we need to specify the dataset, for which we want to extract the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe661fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from flx.data.dataset import *\n",
    "from flx.data.image_loader import SFingeLoader\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute path to the notebooks/example-dataset directory here! \n",
    "DATASET_PATH: str = os.path.abspath(\"/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/FVC2006_DB3_A\")\n",
    "\n",
    "# We will use the SFingeLoader to load the images from the dataset\n",
    "image_loader = TransformedImageLoader(\n",
    "        images=SFingeLoader(DATASET_PATH),\n",
    "        poses=None,\n",
    "        transforms=[\n",
    "            LazilyAllocatedBinarizer(5.0),\n",
    "            pad_and_resize_to_deepprint_input_size,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "image_dataset: Dataset = Dataset(image_loader, image_loader.ids)\n",
    "\n",
    "# The second value is for the minutiae branch, which we do not have in this example\n",
    "texture_embeddings, minutia_embeddings = extractor.extract(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.benchmarks.matchers import CosineSimilarityMatcher\n",
    "from flx.data.embedding_loader import EmbeddingLoader\n",
    "\n",
    "# We concatenate texture and minutia embedding vectors\n",
    "embeddings = EmbeddingLoader.combine(texture_embeddings, minutia_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dfa1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def gray_code(n):\n",
    "    return n ^ (n >> 1)\n",
    "\n",
    "def convert_to_gray_code(x, y):\n",
    "    gray_x = gray_code(int(x))\n",
    "    gray_y = gray_code(int(y))\n",
    "    return bin(gray_x)[2:], bin(gray_y)[2:]\n",
    "\n",
    "def pad_binary(binary, length):\n",
    "    return binary.zfill(length)\n",
    "\n",
    "def process_txt_file(txt_file, output_folder):\n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        x, y = map(float, lines[0].split())\n",
    "        \n",
    "        gray_x, gray_y = convert_to_gray_code(x, y)\n",
    "        \n",
    "        concatenated_binary = gray_x + gray_y\n",
    "        \n",
    "        max_length = len(concatenated_binary)\n",
    "        \n",
    "        padded_binary = pad_binary(concatenated_binary, max_length)\n",
    "        \n",
    "        output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(txt_file))[0] + '_result.txt')\n",
    "        with open(output_file, 'w') as out_f:\n",
    "            out_f.write(padded_binary)\n",
    "\n",
    "def process_folder(input_folder, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            process_txt_file(os.path.join(input_folder, filename), output_folder)\n",
    "\n",
    "# Specify input and output folders\n",
    "input_folder = '/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/walking_core_points'\n",
    "output_folder = '/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/walking_binary_core'\n",
    "\n",
    "# Process all .txt files in the input folder\n",
    "process_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45db329",
   "metadata": {},
   "source": [
    "For Fingerflow convert core to binary using xor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def convert_to_binary(x, y):\n",
    "    binary_x = bin(int(x))[2:]\n",
    "    binary_y = bin(int(y))[2:]\n",
    "    return binary_x, binary_y\n",
    "\n",
    "def pad_binary(binary, length):\n",
    "    return binary.zfill(length)\n",
    "\n",
    "def xor_binary(binary1, binary2):\n",
    "    return bin(int(binary1, 2) ^ int(binary2, 2))[2:]\n",
    "\n",
    "def process_txt_file(txt_file):\n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        x1, y1 = map(float, lines[0].split())\n",
    "        x2, y2 = map(float, lines[1].split())\n",
    "        \n",
    "        binary_x1, binary_y1 = convert_to_binary(x1, y1)\n",
    "        binary_x2, binary_y2 = convert_to_binary(x2, y2)\n",
    "        \n",
    "        s1 = binary_x1 + binary_y1\n",
    "        s2 = binary_x2 + binary_y2\n",
    "        \n",
    "        max_length = max(len(s1), len(s2))\n",
    "        s1 = pad_binary(s1, max_length)\n",
    "        s2 = pad_binary(s2, max_length)\n",
    "        \n",
    "        result = xor_binary(s1, s2)\n",
    "        \n",
    "        output_file = os.path.splitext(txt_file)[0] + '_result.txt'\n",
    "        with open(output_file, 'w') as out_f:\n",
    "            out_f.write(result)\n",
    "\n",
    "def process_folder(input_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            process_txt_file(os.path.join(input_folder, filename))\n",
    "\n",
    "# Specify input folder containing .txt files\n",
    "input_folder = '/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/fixedlength-core_points2'\n",
    "\n",
    "# Process all .txt files in the folder\n",
    "process_folder(input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4910be",
   "metadata": {},
   "source": [
    "Random projection and all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3b5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Define z-score normalization function\n",
    "# def z_score_normalize(data):\n",
    "#     mean_val = np.mean(data)\n",
    "#     std_dev = np.std(data)\n",
    "#     normalized_data = (data - mean_val) / std_dev\n",
    "#     return normalized_data\n",
    "\n",
    "# # Define function to apply median filter\n",
    "# def median_filter(data):\n",
    "#     # Apply median filter to the entire array\n",
    "#     filtered_data = np.median(data)\n",
    "#     print(filtered_data)\n",
    "#     return filtered_data\n",
    "\n",
    "# # Define function to apply random projection\n",
    "# def random_projection(data, n_components):\n",
    "#     # For 1D arrays, random projection doesn't make sense, but you can just return the original array\n",
    "#     return data\n",
    "\n",
    "# # Define function to convert normalized array to binary array\n",
    "# def to_binary_array(data, threshold):\n",
    "#     return (data >= threshold).astype(int)\n",
    "\n",
    "# # Choose normalization method and threshold\n",
    "# normalize_method = z_score_normalize  # Choose either min_max_scale or z_score_normalize\n",
    "# threshold = 0.3\n",
    "\n",
    "# # List to store binary arrays\n",
    "# binary_arrays = []\n",
    "\n",
    "# # Normalize, filter, and project each array\n",
    "# for array in texture_embeddings._array:\n",
    "#     # Normalize data\n",
    "#     normalized_array = normalize_method(array)\n",
    "    \n",
    "#     # Apply median filter\n",
    "#     filtered_array = median_filter(normalized_array)\n",
    "    \n",
    "#     # # Apply random projection\n",
    "#     # projected_array = random_projection(filtered_array, n_components=10)  # Adjust n_components as needed\n",
    "    \n",
    "#     # Convert to binary array\n",
    "#     binary_array = to_binary_array(filtered_array, threshold)\n",
    "    \n",
    "#     # Append to list\n",
    "#     binary_arrays.append(binary_array)\n",
    "\n",
    "# # Print binary arrays\n",
    "# for i, binary_array in enumerate(binary_arrays):\n",
    "#     print(f\"Binary array for array {i}:\")\n",
    "#     print(binary_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc4054",
   "metadata": {},
   "source": [
    "Hardcored Core points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251db520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Define z-score normalization function\n",
    "# def z_score_normalize(data):\n",
    "#     mean_val = np.mean(data)\n",
    "#     std_dev = np.std(data)\n",
    "#     normalized_data = (data - mean_val) / std_dev\n",
    "#     return normalized_data\n",
    "\n",
    "# # Define function to convert normalized array to binary array\n",
    "# def to_binary_array(data, threshold):\n",
    "#     return (data >= threshold).astype(int)\n",
    "\n",
    "# # Choose normalization method and threshold\n",
    "# normalize_method = z_score_normalize  # Choose either min_max_scale or z_score_normalize\n",
    "# threshold = 0.5\n",
    "\n",
    "# # List to store binary arrays\n",
    "# binary_arrays = []\n",
    "\n",
    "# # Normalize and convert each array to binary\n",
    "# for array in texture_embeddings._array:\n",
    "#     # Sort the array (not sure if this step is necessary based on your code)\n",
    "#     sorted(array)\n",
    "\n",
    "#     # Normalize the array\n",
    "#     normalized_array = normalize_method(array)\n",
    "    \n",
    "#     # Convert the normalized array to binary using the threshold\n",
    "#     binary_array = to_binary_array(normalized_array, threshold)\n",
    "    \n",
    "#     # Append the binary array to the list\n",
    "#     binary_arrays.append(binary_array)\n",
    "\n",
    "# # Generate the permutation pattern from core points (assuming core_points is available)\n",
    "# core_points = [0, 1, 0, 1]  # Example core points as a binary array\n",
    "\n",
    "# # Function to permute binary array in intervals using a pattern\n",
    "# def permute_binary_array_in_intervals(binary_array, pattern):\n",
    "#     permuted_array = []\n",
    "#     interval_length = len(binary_array) // len(pattern)\n",
    "#     for i in range(0, len(binary_array), interval_length):\n",
    "#         segment = binary_array[i:i+interval_length]\n",
    "#         permuted_segment = [segment[j] for j in pattern]\n",
    "#         permuted_array.extend(permuted_segment)\n",
    "#     return np.array(permuted_array)\n",
    "\n",
    "# # Permute all binary arrays using the generated pattern\n",
    "# permuted_arrays = []\n",
    "# for binary_array in binary_arrays:\n",
    "#     permuted_binary_array = permute_binary_array_in_intervals(binary_array, core_points)\n",
    "#     permuted_arrays.append(permuted_binary_array)\n",
    "\n",
    "\n",
    "# # Print the permuted binary arrays\n",
    "# for i, permuted_binary_array in enumerate(permuted_arrays):\n",
    "#     print(f\"Permuted binary array for array {i}:\")\n",
    "#     print(permuted_binary_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af280f",
   "metadata": {},
   "source": [
    "This is the permutation with core as binary value seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define z-score normalization function\n",
    "def z_score_normalize(data):\n",
    "    mean_val = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    normalized_data = (data - mean_val) / std_dev\n",
    "    return normalized_data\n",
    "\n",
    "# Define function to convert normalized array to binary array\n",
    "def to_binary_array(data, threshold):\n",
    "    return (data >= threshold).astype(int)\n",
    "\n",
    "# Choose normalization method and threshold\n",
    "normalize_method = z_score_normalize  # Choose either min_max_scale or z_score_normalize\n",
    "threshold = 0.3\n",
    "\n",
    "# List to store binary arrays\n",
    "binary_arrays = []\n",
    "\n",
    "# Normalize and convert each array to binary\n",
    "for array in embeddings._array:\n",
    "    # Sort the array (not sure if this step is necessary based on your code)\n",
    "    sorted(array)\n",
    "\n",
    "    # Normalize the array\n",
    "    normalized_array = normalize_method(array)\n",
    "    \n",
    "    # Convert the normalized array to binary using the threshold\n",
    "    binary_array = to_binary_array(normalized_array, threshold)\n",
    "    \n",
    "    # Append the binary array to the list\n",
    "    binary_arrays.append(binary_array)\n",
    "len(binary_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Folder containing core point files\n",
    "pose_points_folder = \"/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/example-dataset-fvc2006db3a_pose-binary\"\n",
    "\n",
    "# Permute all binary arrays using the core points from files in the folder\n",
    "permuted_array = []\n",
    "\n",
    "for binary_array in binary_arrays:\n",
    "    for core_points_file in sorted(os.listdir(core_points_folder)):\n",
    "    # Read core points from the current file\n",
    "        with open(os.path.join(core_points_folder, core_points_file), \"r\") as f:\n",
    "            core_points_str = f.read().strip()\n",
    "\n",
    "\n",
    "    # Convert the core points string to a list of integers\n",
    "        core_points = [int(bit) for bit in core_points_str]\n",
    "\n",
    "    # Function to permute binary array in intervals using the pattern\n",
    "    def permute_binary_array_in_intervals(binary_array, pattern):\n",
    "        permuted_array = []\n",
    "        interval_length = len(binary_array) // len(pattern)\n",
    "        for i in range(0, len(binary_array), interval_length):\n",
    "            segment = binary_array[i:i+interval_length]\n",
    "            permuted_segment = [segment[j] for j in pattern]\n",
    "            permuted_array.extend(permuted_segment)\n",
    "        return np.array(permuted_array)\n",
    "\n",
    "    # Permute the binary array using the read core points\n",
    "    permuted_binary_array = permute_binary_array_in_intervals(binary_array, core_points)\n",
    "    permuted_array.append(permuted_binary_array)\n",
    "\n",
    "# Print the permuted binary arrays\n",
    "for i, permuted_binary_array in enumerate(permuted_array):\n",
    "    print(f\"Permuted binary array for array {i}:\")\n",
    "    print(permuted_binary_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8cbc4",
   "metadata": {},
   "source": [
    "This is for tensors to normalise and with threshold get binary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing core point files\n",
    "pose_points_folder = \"/home/mt0/22CS60R42/fixed-length-fingerprint-extractors/notebooks/example-dataset-fvc2006db3a_pose-binary\"\n",
    "permuted_array = []\n",
    "pose_point_patterns = []\n",
    "for core_points_file in sorted(os.listdir(pose_points_folder)):\n",
    "    # Read core points from the current file\n",
    "        with open(os.path.join(pose_points_folder, core_points_file), \"r\") as f:\n",
    "            core_points_str = f.read().strip()\n",
    "            # print(core_points_str)\n",
    "            pose_point_patterns.append(core_points_str)\n",
    "\n",
    "len(pose_point_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32963ce2",
   "metadata": {},
   "source": [
    "For permutation one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(binary, pattern):\n",
    "    # Convert binary and pattern strings into lists for easier manipulation\n",
    "    binary_list = list(binary)\n",
    "    pattern_list = list(pattern)\n",
    "    \n",
    "    # Get the lengths of binary and pattern lists\n",
    "    n = len(binary_list)\n",
    "    m = len(pattern_list)\n",
    "\n",
    "    # Initialize variables\n",
    "    result = []         # To store the final result\n",
    "    skipped_bits = []   # To store bits skipped based on the pattern\n",
    "    j = 0               # Pointer to track the current position in the pattern\n",
    "\n",
    "    # Iterate through each bit in the binary string\n",
    "    for i in range(n):\n",
    "        if pattern_list[j] == '1':  \n",
    "            # If the current pattern bit is '1', add the corresponding binary bit to the result\n",
    "            result.append(str(binary_list[i]))  # Ensure this is a string\n",
    "        else:\n",
    "            # If the current pattern bit is '0', skip the binary bit and store it in skipped_bits\n",
    "            skipped_bits.append(str(binary_list[i]))  # Ensure this is a string\n",
    "        \n",
    "        # Move to the next pattern bit\n",
    "        j += 1  \n",
    "        \n",
    "        # Reset the pattern pointer when the end of the pattern is reached\n",
    "        if j == m:\n",
    "            j = 0\n",
    "            \n",
    "    # Append all skipped bits to the result at the end\n",
    "    result.extend(str(bit) for bit in skipped_bits)  # Convert skipped bits to strings\n",
    "    \n",
    "    # Convert the result list back into a string and return it\n",
    "    return ''.join(result)\n",
    "\n",
    "# Function to handle lists of binary and pattern strings\n",
    "def transform_all(binaries, patterns):\n",
    "    # Apply the transform function to each pair of binary and pattern\n",
    "    results = [\n",
    "        transform(binary, pattern) \n",
    "        for binary, pattern in zip(binaries, patterns)\n",
    "    ]\n",
    "    return results\n",
    "\n",
    "# Example input\n",
    "binary = binary_arrays\n",
    "pattern = pose_point_patterns\n",
    "\n",
    "# Apply the transformation to each binary-pattern pair\n",
    "result = transform_all(binary, pattern)\n",
    "\n",
    "# Print the results\n",
    "# print(len(result))\n",
    "for x in result:\n",
    "    print(\"OUTPUT\",x)\n",
    "\n",
    "\n",
    "# def hamming_distance_numpy(x, y):\n",
    "#     # Ensure both strings are of the same length\n",
    "#     assert len(x) == len(y), \"Binary strings must be of the same length\"\n",
    "#     return np.sum(np.array(x) != np.array(y))\n",
    "\n",
    "def hamming_dist(s1, s2):\n",
    "    if len(s1) != len(s2):\n",
    "        return -1\n",
    "    return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n",
    "\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "hamming_distance_matrix = np.zeros((num_rows, num_cols), dtype=int)\n",
    "\n",
    "for i in range(num_rows):\n",
    "    x = result[i]\n",
    "    for j in range(num_cols):\n",
    "        y = result[j]\n",
    "        hamming_distance_matrix[i, j] = hamming_dist(x, y)\n",
    "\n",
    "\n",
    "for x in hamming_distance_matrix:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cea2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_dist(s1, s2):\n",
    "    if len(s1) != len(s2):\n",
    "        return -1\n",
    "    return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n",
    "\n",
    "# driver code\n",
    "for i in range(len(result)):\n",
    "    for j in range(len(result)):\n",
    "        print(\"hamming distance between\", f\"{i}\" and f\"{j}\",f\"{hamming_dist(result[i], result[j]):=}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Hamming distance between two binary arrays\n",
    "def hamming_distance(array1, array2):\n",
    "    return np.sum(array1 != array2)\n",
    "\n",
    "# Initialize lists to store binary arrays for each person\n",
    "binary_arrays_same_person = []\n",
    "binary_arrays_diff_person = []\n",
    "\n",
    "\n",
    "\n",
    "# Populate binary arrays lists\n",
    "for i, permuted_binary_array in enumerate(permuted_array):\n",
    "    # Append to appropriate list based on person index\n",
    "    person_index = i // 10  # Assuming each person has 10 impressions\n",
    "    if person_index < 6:\n",
    "        binary_arrays_same_person.append(permuted_binary_array)\n",
    "    else:\n",
    "        binary_arrays_diff_person.append(permuted_binary_array)\n",
    "\n",
    "# Calculate Hamming distance for same person pairs\n",
    "print(\"Hamming distances for same person pairs:\")\n",
    "for i in range(len(binary_arrays_same_person)):\n",
    "    for j in range(i + 1, len(binary_arrays_same_person)):\n",
    "        distance = hamming_distance(binary_arrays_same_person[i], binary_arrays_same_person[j])\n",
    "        print(f\"Impression {i} vs Impression {j}: {distance}\")\n",
    "\n",
    "# Calculate Hamming distance for different person pairs\n",
    "print(\"\\nHamming distances for different person pairs:\")\n",
    "for i in range(len(binary_arrays_same_person)):\n",
    "    for j in range(len(binary_arrays_diff_person)):\n",
    "        distance = hamming_distance(binary_arrays_same_person[i], binary_arrays_diff_person[j])\n",
    "        print(f\"Impression {i} (Same Person) vs Impression {j} (Different Person): {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94b892",
   "metadata": {},
   "source": [
    "Hamming distance code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate Hamming distance between two binary arrays\n",
    "def hamming_distance(array1, array2):\n",
    "    return np.sum(array1 != array2)\n",
    "\n",
    "# Initialize lists to store binary arrays for each person\n",
    "binary_arrays_same_person = []\n",
    "binary_arrays_diff_person = []\n",
    "\n",
    "# Normalize and convert each array to binary\n",
    "for array in result:\n",
    "    # normalized_array = normalize_method(array)\n",
    "    # binary_array = to_binary_array(normalized_array, threshold)\n",
    "    binary_array = array\n",
    "    # Append to appropriate list based on person index\n",
    "    person_index = i % 12  # Assuming each person has 10 impressions\n",
    "    if person_index < 12:\n",
    "        binary_arrays_same_person.append(binary_array)\n",
    "    else:\n",
    "        binary_arrays_diff_person.append(binary_array)\n",
    "\n",
    "# Calculate Hamming distance for same person pairs\n",
    "print(\"Hamming distances for same person pairs:\")\n",
    "for i in range(len(binary_arrays_same_person)):\n",
    "    for j in range(i + 1, len(binary_arrays_same_person)):\n",
    "        distance = hamming_distance(binary_arrays_same_person[i], binary_arrays_same_person[j])\n",
    "        print(f\"Impression {i} vs Impression {j}: {distance}\")\n",
    "\n",
    "# Calculate Hamming distance for different person pairs\n",
    "print(\"\\nHamming distances for different person pairs:\")\n",
    "for i in range(len(binary_arrays_diff_person)):\n",
    "    for j in range(len(binary_arrays_diff_person)):\n",
    "        distance = hamming_distance(binary_arrays_diff_person[i], binary_arrays_diff_person[j])\n",
    "        print(f\"Impression {i} vs Impression {j}: {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d96af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hamming_distance_numpy(x, y):\n",
    "#     # Ensure both strings are of the same length\n",
    "#     assert len(x) == len(y), \"Binary strings must be of the same length\"\n",
    "#     return np.sum(np.array(x) != np.array(y))\n",
    "\n",
    "def hamming_dist(s1, s2):\n",
    "    if len(s1) != len(s2):\n",
    "        return -1\n",
    "    return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n",
    "\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "hamming_distance_matrix = np.zeros((num_rows, num_cols), dtype=int)\n",
    "\n",
    "for i in range(num_rows):\n",
    "    x = binary_arrays[i]\n",
    "    for j in range(num_cols):\n",
    "        y = binary_arrays[j]\n",
    "        hamming_distance_matrix[i, j] = hamming_dist(x, y)\n",
    "\n",
    "\n",
    "for x in hamming_distance_matrix:\n",
    "    print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f9c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c15f7d98",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the embeddings, we want to run a benchmark on them. For this, we must first specify the type of benchmark, and which comparisons should be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.scripts.generate_benchmarks import create_verification_benchmark\n",
    "\n",
    "NUM_IMPRESSIONS_PER_SUBJECT = 10\n",
    "benchmark = create_verification_benchmark(\n",
    "    #subjects=list(range(100, image_dataset.num_subjects + 100)),\n",
    "    subjects=list(range(image_dataset.num_subjects)),\n",
    "    impressions_per_subject=list(range(NUM_IMPRESSIONS_PER_SUBJECT))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a37e9ef",
   "metadata": {},
   "source": [
    "Now we can run the benchmark. To do this, we must first specify the matcher (in our case cosine similarity of the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.benchmarks.matchers import CosineSimilarityMatcher\n",
    "from flx.data.embedding_loader import EmbeddingLoader\n",
    "\n",
    "# We concatenate texture and minutia embedding vectors\n",
    "embeddings = EmbeddingLoader.combine(texture_embeddings, minutia_embeddings)\n",
    "matcher = CosineSimilarityMatcher(EmbeddingLoader.combine(texture_embeddings, minutia_embeddings))\n",
    "\n",
    "results = benchmark.run(matcher)\n",
    "\n",
    "print(f\"Equal-Error-Rate: {results.get_equal_error_rate()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5ee4287",
   "metadata": {},
   "source": [
    "To visualize the results, we can plot a DET curve. (Do not wonder if it is empty, probably the model is not trained enough. Take a look at the EER instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.visualization.plot_DET_curve import plot_verification_results\n",
    "\n",
    "figure_path = \"DET_curve\"\n",
    "\n",
    "# Lists are used to allow for multiple models to be plotted in the same figure\n",
    "plot_verification_results(figure_path, results=[results], model_labels=[\"DeepPrint_TexMinu\"], plot_title=\"example-dataset - verification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
