{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f0c027",
   "metadata": {},
   "source": [
    "In this tutorial we will learn to:\n",
    "- Instantiate a DeepPrintExtractor\n",
    "- Prepare a training dataset\n",
    "- Train a DeepPrintExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b5048",
   "metadata": {},
   "source": [
    "## Instantiate a DeepPrintExtractor\n",
    "\n",
    "This package implements a number of variants of the DeepPrint architecture. The wrapper class for all these variants is called `DeepPrintExtractor`.\n",
    "It has a `fit` method to train (and save) the model as well as an `extract` method to extract the DeepPrint features for fingerprint images. \n",
    "\n",
    "You can also try to implement your own models, but currently this is not directly supported by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cdc3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 10 subjects and a total of 80 samples.\n"
     ]
    }
   ],
   "source": [
    "from flx.data.dataset import IdentifierSet, Identifier\n",
    "from flx.extractor.fixed_length_extractor import get_DeepPrint_TexMinu,get_DeepPrint_Minu,get_DeepPrint_LocTex, DeepPrintExtractor\n",
    "\n",
    "# We will use the example dataset with 10 subjects and 10 impression per subject\n",
    "training_ids: IdentifierSet = IdentifierSet([Identifier(i, j) for i in range(100,110) for j in range(8)])\n",
    "\n",
    "# We choose a dimension of 512 for the fixed-length representation (TexMinu has two outputs num_dims)\n",
    "extractor: DeepPrintExtractor = get_DeepPrint_LocTex(num_training_subjects=training_ids.num_subjects, num_texture_dims=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ba71af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flx.data.dataset.IdentifierSet at 0x7f0cafe1f190>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9687ae",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Instantiating the model was easy. To train it, first we will load the training data (see the [data tutorial](./dataset_tutorial.ipynb) for how to implement your own dataset).\n",
    "\n",
    "Besides the fingerprint images, we also need a mapping from subjects to integer labels (for pytorch). For some variants we also need minutiae data. To see how a more complex dataset can be loaded, have a look at `flx/setup/datasets.py`.\n",
    "\n",
    "Finally, we call the `fit` method, which trains the model and saves it to the specified path.\n",
    "\n",
    "There is also the option to add a validation set, which will be used to evaluate the embeddings during training. This is useful to monitor the training progress and to avoid overfitting.\n",
    "In this example we will not use a validation set for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf461144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 10 subjects and a total of 80 samples.\n",
      "Created IdentifierSet with 10 subjects and a total of 80 samples.\n",
      "alka\n",
      "Using device cpu\n",
      "No model file found at /home/rs/21CS91R01/research/fixed-length-fingerprint-extractors/notebooks/trial_model/model.pyt\n",
      "\n",
      "\n",
      " --- Starting Epoch 1 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:20<00:00, 28.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1604164.1298899048\n",
      "Multiclass accuracy: 0.125\n",
      "TrainingLogEntry(\n",
      "    epoch=1,\n",
      "    training_loss=1604164.1298899048,\n",
      "    loss_statistics={'minutia_loss': {'crossent_loss_sum': 0.23300681114196778, 'center_loss_sum': 0.22791186571121216}, 'minutia_map_loss': 100259.79719943623},\n",
      "    training_accuracy=0.125,\n",
      "    validation_equal_error_rate=0.125,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 2 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:42<00:00, 32.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 190913.8847218323\n",
      "Multiclass accuracy: 0.03750000149011612\n",
      "TrainingLogEntry(\n",
      "    epoch=2,\n",
      "    training_loss=190913.8847218323,\n",
      "    loss_statistics={'minutia_loss': {'crossent_loss_sum': 0.08071020245552063, 'center_loss_sum': 0.09443754255771637}, 'minutia_map_loss': 5965.883749809265},\n",
      "    training_accuracy=0.03750000149011612,\n",
      "    validation_equal_error_rate=0.03750000149011612,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 3 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:48<00:00, 33.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 53685.13687746048\n",
      "Multiclass accuracy: 0.07500000298023224\n",
      "TrainingLogEntry(\n",
      "    epoch=3,\n",
      "    training_loss=53685.13687746048,\n",
      "    loss_statistics={'minutia_loss': {'crossent_loss_sum': 0.055977290868759154, 'center_loss_sum': 0.05175041953722636}, 'minutia_map_loss': 1118.332623901367},\n",
      "    training_accuracy=0.07500000298023224,\n",
      "    validation_equal_error_rate=0.07500000298023224,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 4 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:57<00:00, 23.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 32334.814621772763\n",
      "Multiclass accuracy: 0.11249999701976776\n",
      "TrainingLogEntry(\n",
      "    epoch=4,\n",
      "    training_loss=32334.814621772763,\n",
      "    loss_statistics={'minutia_loss': {'crossent_loss_sum': 0.041099635511636735, 'center_loss_sum': 0.032095634192228314}, 'minutia_map_loss': 505.1582831954955},\n",
      "    training_accuracy=0.11249999701976776,\n",
      "    validation_equal_error_rate=0.11249999701976776,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 5 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:41<00:00, 32.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 32432.93615842819\n",
      "Multiclass accuracy: 0.05000000074505806\n",
      "TrainingLogEntry(\n",
      "    epoch=5,\n",
      "    training_loss=32432.93615842819,\n",
      "    loss_statistics={'minutia_loss': {'crossent_loss_sum': 0.031037132143974303, 'center_loss_sum': 0.021133530139923095}, 'minutia_map_loss': 405.3595313186645},\n",
      "    training_accuracy=0.05000000074505806,\n",
      "    validation_equal_error_rate=0.05000000074505806,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch \n",
    "\n",
    "from flx.data.dataset import *\n",
    "from flx.data.image_loader import SFingeLoader\n",
    "from flx.data.minutia_map_loader import SFingeMinutiaMapLoader\n",
    "from flx.data.label_index import LabelIndex\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute paths manually here! \n",
    "DATASET_DIR: str = os.path.abspath(\"fvc_dataset/fvc2002_DB3_B capacitive/converted\")\n",
    "MODEL_OUTDIR: str = os.path.abspath(\"trial_model\")\n",
    "\n",
    "# We will use the SFingeLoader to load the images from the dataset\n",
    "image_loader = TransformedImageLoader(\n",
    "        images=SFingeLoader(DATASET_DIR),\n",
    "        poses=None,\n",
    "        transforms=[\n",
    "            LazilyAllocatedBinarizer(5.0),\n",
    "            pad_and_resize_to_deepprint_input_size,\n",
    "        ],\n",
    "    )\n",
    "# print(training_ids.num_subjects)\n",
    "\n",
    "image_dataset = Dataset(image_loader, training_ids)\n",
    "\n",
    "# For pytorch, we need to map the subjects to integer labels from [0 ... num_subjects-1]\n",
    "label_dataset = Dataset(LabelIndex(training_ids), training_ids)\n",
    "\n",
    "minutia_maps_dataset = Dataset(SFingeMinutiaMapLoader(DATASET_DIR), training_ids)\n",
    "print(\"alka\")\n",
    "extractor.fit(\n",
    "    fingerprints=image_dataset,\n",
    "    minutia_maps=minutia_maps_dataset,\n",
    "    labels=label_dataset,\n",
    "    validation_fingerprints=None,\n",
    "    validation_benchmark=None,\n",
    "    num_epochs=5,\n",
    "    out_dir=MODEL_OUTDIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1a7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 10 subjects and a total of 80 samples.\n",
      "Created IdentifierSet with 10 subjects and a total of 80 samples.\n",
      "alka\n",
      "Using device cpu\n",
      "No model file found at /home/rs/21CS91R01/research/fixed-length-fingerprint-extractors/notebooks/loc_tex/model.pyt\n",
      "\n",
      "\n",
      " --- Starting Epoch 1 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:09<00:00, 61.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 5.457501411437988\n",
      "Multiclass accuracy: 0.05000000074505806\n",
      "TrainingLogEntry(\n",
      "    epoch=1,\n",
      "    training_loss=5.457501411437988,\n",
      "    loss_statistics={'crossent_loss_sum': 0.14773398041725158, 'center_loss_sum': 0.19335986077785491},\n",
      "    training_accuracy=0.05000000074505806,\n",
      "    validation_equal_error_rate=0.05000000074505806,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 2 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:03<00:00, 60.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 4.7597064018249515\n",
      "Multiclass accuracy: 0.05000000074505806\n",
      "TrainingLogEntry(\n",
      "    epoch=2,\n",
      "    training_loss=4.7597064018249515,\n",
      "    loss_statistics={'crossent_loss_sum': 0.07405644208192826, 'center_loss_sum': 0.07468437999486924},\n",
      "    training_accuracy=0.05000000074505806,\n",
      "    validation_equal_error_rate=0.05000000074505806,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 3 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:22<00:00, 64.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 4.375380802154541\n",
      "Multiclass accuracy: 0.0625\n",
      "TrainingLogEntry(\n",
      "    epoch=3,\n",
      "    training_loss=4.375380802154541,\n",
      "    loss_statistics={'crossent_loss_sum': 0.04918948113918305, 'center_loss_sum': 0.04196428606907527},\n",
      "    training_accuracy=0.0625,\n",
      "    validation_equal_error_rate=0.0625,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 4 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:53<00:00, 58.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 4.029953575134277\n",
      "Multiclass accuracy: 0.11249999701976776\n",
      "TrainingLogEntry(\n",
      "    epoch=4,\n",
      "    training_loss=4.029953575134277,\n",
      "    loss_statistics={'crossent_loss_sum': 0.03668036088347435, 'center_loss_sum': 0.026287664100527762},\n",
      "    training_accuracy=0.11249999701976776,\n",
      "    validation_equal_error_rate=0.11249999701976776,\n",
      "}\n",
      "\n",
      "\n",
      " --- Starting Epoch 5 of 5 ---\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:22<00:00, 52.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 3.792448043823242\n",
      "Multiclass accuracy: 0.07500000298023224\n",
      "TrainingLogEntry(\n",
      "    epoch=5,\n",
      "    training_loss=3.792448043823242,\n",
      "    loss_statistics={'crossent_loss_sum': 0.029407902359962462, 'center_loss_sum': 0.017997698187828065},\n",
      "    training_accuracy=0.07500000298023224,\n",
      "    validation_equal_error_rate=0.07500000298023224,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch \n",
    "\n",
    "from flx.data.dataset import *\n",
    "from flx.data.image_loader import SFingeLoader\n",
    "from flx.data.minutia_map_loader import SFingeMinutiaMapLoader\n",
    "from flx.data.label_index import LabelIndex\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute paths manually here! \n",
    "DATASET_DIR: str = os.path.abspath(\"fvc_dataset/fvc2002_DB3_B capacitive/converted\")\n",
    "MODEL_OUTDIR: str = os.path.abspath(\"loc_tex\")\n",
    "\n",
    "# We will use the SFingeLoader to load the images from the dataset\n",
    "image_loader = TransformedImageLoader(\n",
    "        images=SFingeLoader(DATASET_DIR),\n",
    "        poses=None,\n",
    "        transforms=[\n",
    "            LazilyAllocatedBinarizer(5.0),\n",
    "            pad_and_resize_to_deepprint_input_size,\n",
    "        ],\n",
    "    )\n",
    "# print(training_ids.num_subjects)\n",
    "\n",
    "image_dataset = Dataset(image_loader, training_ids)\n",
    "\n",
    "# For pytorch, we need to map the subjects to integer labels from [0 ... num_subjects-1]\n",
    "label_dataset = Dataset(LabelIndex(training_ids), training_ids)\n",
    "\n",
    "minutia_maps_dataset = Dataset(SFingeMinutiaMapLoader(DATASET_DIR), training_ids)\n",
    "print(\"alka\")\n",
    "extractor.fit(\n",
    "    fingerprints=image_dataset,\n",
    "    minutia_maps=minutia_maps_dataset,\n",
    "    labels=label_dataset,\n",
    "    validation_fingerprints=None,\n",
    "    validation_benchmark=None,\n",
    "    num_epochs=5,\n",
    "    out_dir=MODEL_OUTDIR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
